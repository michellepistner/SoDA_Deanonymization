---
title: "Parsing Comments and Feature Extraction"
subtitle: "Code Review"
author: "Michelle Pistner"
date: "November 7, 2017"
output:
   html_document:
    toc: true
    toc_float: true
    theme: readable
    highlight: zenburn 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

My part of our project includes extracting features based off of the parts of speech of a comment. Then, these features will be combined with the features that Amy created, and we will each use separate modelling techniques to model authorship.

For my feature extraction, I'm following the lead of Narayanan, et. al (2012) and Caliskan, et. al (2015). These papers study authorship attribution of large author pools and code fragments, respectively. They both use a similar feature set with slight twists based on their setting.

For this project, the features that we're interested in include:

* The frequency of parts of speech
* The frequency of node pairs in a parse tree
* The frequency of letters, characters, and numbers
* Word capitalization
* Vocabulary richness

I'm focusing on the first two since they both require the use of a language parser. Amy presented code last week to extract the remaining features.

For our project, we are using the Reddit comment data set for February 2017. These files are massive and publically available! They can be downloaded via torrents [here](https://www.reddit.com/r/datasets/comments/65o7py/updated_reddit_comment_dataset_as_torrents/).

## Basics of the Stanford Parser

The [Stanford Parser](https://nlp.stanford.edu/software/lex-parser.shtml) is a natural language processing tool that can be used to extract parts of speech and relationships between words. It can also extract other information about a text, including sentiment and named entities It supports many languages, including English, Spanish, Chinese, and German.

The parser is written in Java, but many programming languages have some sort of wrapper for the parser. For my project, I'm using Python's `pycorenlp`, but many other wrappers exist. There is a (relatively) new R package `coreNLP` that I also found easy to use, but it was slower than Python's version and less easy to manipulate for this project.

If you are using `pycorenlp`, you will need to take steps to start the server for Stanford's parser. The first step is to [download the server](https://nlp.stanford.edu/software/lex-parser.shtml#Download) and [Java](https://java.com/en/download/). Next, you need to actually start the server. This step is straightforward; run the following from the command line in the directory that contains the downloaded server.

```{pyton, eval=FALSE}
java -mx4g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
```

Once the server is up and running, you are ready to parse!

Following Narayanan's example (2012), suppose we want to extract the parse tree and parts of speech for the sentence "Which of these people wrote this blog post?"

This is fairly straightforward to do using the `annotate` command from the `pycorenlp` module. The `properties` definition allows you to specify what specifically the parser should return. Here, we want the parts of speech tagger and the parse tree.

```{python}
import pycorenlp as nlp
nlp = nlp.StanfordCoreNLP('http://localhost:9000')

text="Which of these people wrote this blog post?"
output = nlp.annotate(text, properties={
        'annotators': 'tokenize,pos,parse',
        'outputFormat': 'json',
        'timeout': 10000
        })
type(output)
print(output)

```

This output isn't very readable, but it will do the job for the features we want to extract. This output can be used to make parse trees which show the features we want to extract much more clearly.

```{python}
import pycorenlp as nlp
import nltk
nlp = nlp.StanfordCoreNLP('http://localhost:9000')

text="Which of these people wrote this blog post?"
output = nlp.annotate(text, properties={
        'annotators': 'tokenize,pos,parse',
        'outputFormat': 'json',
        'timeout': 10000
        })

tree=[s['parse'] for s in output['sentences']]
nltk.Tree.fromstring(tree[0]).pretty_print()

```

We want to extract both the total number of occurrences of each part of speech and the parent/child note pairs (i.e. (WHNP, WDT) , (PP,NP)).

All in all, the Stanford Parser outputs 5 clause level labels, 21 phrase level labels, 36 word level labels, and 9 punctuation tags. A list of all labels and their meanings can be found [here](http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html).

##Extracting Features

###Setting up the script

To run this code, several python modules are needed. The first three are used in reading in the data. The remainder are used in the actual feature extraction.

```{python, eval=FALSE}
##Importing the required libraries
import bz2
import glob
import json
import nltk
import pycorenlp as nlp 
from collections import Counter
import sys
import csv
import re
from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')

```
Next, I find all the `bz2` files located in my current directory and specify what subreddit's comments I'm interested in.
```{python, eval=FALSE}
files=glob.glob("*.bz2")
subreddits="nfl"

```

###Defining the features

Next, I defined the feature variable names that we are interested in. The parts of speech names were taken directly from the list of labels output by the Stanford parser. The set of node/child pairs was obtained by permuting all possible clause/phrase/word labels

```{python, eval=FALSE}

partsOfSpeech=("NN","PRP","CC","CD","DT","EX","FW","IN","JJ","JJR","JJS","LS","MD","NNS","NNP","NNPS","PDT","POS","PRP$","RB","RBR","RBS","RP","SYM","TO","UH","VB","VBD","VBG","VBN","VBP","VBZ","WDT","WP","WP$","WRB")
phrases=(('S', 'ADJP'),('S', 'ADVP'),('S', 'CONJP'),('S', 'FRAG'),('S', 'INTJ'),('S', 'LST'), ('S', 'NAC'), ('S', 'NP'),('S', 'NX'),('S', 'PP'),('S', 'PRN'), ('S', 'PRT'), ('S', 'QP'), ('S', 'RRC'), ('S', 'UCP'), ('S', 'VP'),...)
pos_col = list(map(lambda x: "pos_{}".format(x), partsOfSpeech))
phr_col = list(map(lambda x: "phr_{}".format(x), phrases))
other_vars=["id","author","subreddit"]
```


###Feature extraction

Extracting parts of speech is straightforward with the output that `pycorenlp` provides. However, to extract node pairs, we need to do a little legwork. We need to declare a function that finds all the dependent pairs and formats them correctly. One option would be to traverse the tree starting at its terminal leaves, but this might involve double counting of pairs higher up in the tree. Instead, I use the `productions` function from the `nltk` module. This function returns something of the form `SBARQ -> WHNP SQ .`---i.e. every parent node and its children. This can then be manipulate to get pairs of the form `(SBARQ, WHNP), (SBARQ, SQ), (SBARQ, .)`.

```{python, eval=FALSE}

def PhrasePairs(tmp):
    tmp=tmp.productions()
    
    lst=[]
    for objects in tmp:
        objects=str(objects)
        objects=objects.split(" -> ")
        if('"' in str(objects)):
            continue
        lst.append(objects)
    
    pairs=[]
    for items in lst:
        parent=str(items[0])
        children=str(items[1]).split(" ")
        for child in children:
            hold="('"+parent +"', '" +child+"')"
            pairs.append(hold)
    return pairs
```

Now, we are ready to extract features. First we begin by reading the comments line by line and filtering out deleted comments or those not in the desired subreddit list.

```{python, eval=FALSE}

with bz2.BZ2File(files[0],mode="r") as f: ##Have to read line by line
    for line in f:
        tmp = f.readline().decode('utf8')
        comment=json.loads(tmp)
        print(comment['body'])
        ##We don't want to include deleted authors or deleted body text.
        if comment['author'] == "[deleted]":
            continue
        if comment['body'] == "[deleted]":
            continue
        if subreddits is None:
            continue
        if comment['subreddit'].lower() not in subreddits: ##Only posts that are in our subreddit
            continue
        if len(comment['body'])<100:
            continue
```

Then, once these comments are filtered out, some light preprocessing is conducted by removing URLs. Then, each comment is parsed using the `annotate` function of `pycorenlp`.

```{python,eval=FALSE}
        text= re.sub(r'http\S+', '', comment['body'])
        output = nlp.annotate(text, properties={
                'annotators': 'tokenize,pos,parse',
                'outputFormat': 'json',
                'timeout': 10000
                })
```

Sometimes, the parser will time out for long or complex comments. We can filter these out easily since output will be a `str` object instead of a `dict` object. Then, we extract the parts of speech and write it to a dictionary. We do this by first iterating over the sentences and extracting the parts of speech for each token. Then, we count the number of each of the defined labels from above and write these counts to a dictionary.

```{python,eval=FALSE}
        if isinstance(output,str):
            continue
        pos=()
        for s in output["sentences"]:
            s=[t["pos"] for t in s["tokens"]]
            pos=pos+(tuple(s))
        PartsSpeech = ["pos_{}".format(word) for word in pos if word in partsOfSpeech]
        pos_count=Counter(PartsSpeech) 
        rowdict = {key: value for key, value in pos_count.items()}
```

We do a similar procedure to extract parent/child pairs. First, for each sentence parse, we use the `tails` function to extract the parent/child pairs. Then, we count the number of each of the pairs from the list defined above. Then, we update the dictionary that we already created.
```{python, eval=FALSE}
        tmp=()
        for s in output["sentences"]:
            tmp=tmp+ (tuple(PhrasePairs(nltk.Tree.fromstring(s["parse"]))))
            
        Phrases = ["phr_{}".format(phrase) for phrase in tmp if phrase in phrases]
        phrase_count=Counter(Phrases)
        rowdict.update(phrase_count)
        
```

Finally, we extract the  ID, author, and subreddit for each comment so these features can be merged later with other features that count the prevalence of characters, punctuation marks, feature words, etc.

Then, we write this dictionary to the file we declared at the beginning of the code.
```{python,eval=FALSE}
        features={"id":comment['id'],"author":comment['author'],"subreddit":comment['subreddit']}
        rowdict.update(features)
        fwriter.writerow(rowdict)
        
```

##Feature Verification

In order to make sure the features were extracted as desired, I used a handful of test cases. I'll show two of them here.

First, we will return to the sentence from before.
```{python}
def PhrasePairs(tmp):
    tmp=tmp.productions()
    
    lst=[]
    for objects in tmp:
        objects=str(objects)
        objects=objects.split(" -> ")
        if('"' in str(objects)):
            continue
        lst.append(objects)
    
    pairs=[]
    for items in lst:
        parent=str(items[0])
        children=str(items[1]).split(" ")
        for child in children:
            hold="('"+parent +"', '" +child+"')"
            pairs.append(hold)
    return pairs
    
import pycorenlp as nlp
import nltk
nlp = nlp.StanfordCoreNLP('http://localhost:9000')

text="Which of these people wrote this blog post?"
output = nlp.annotate(text, properties={
        'annotators': 'tokenize,pos,parse',
        'outputFormat': 'json',
        'timeout': 10000
        })

tree=[s['parse'] for s in output['sentences']]
nltk.Tree.fromstring(tree[0]).pretty_print()

pos=()
for s in output["sentences"]:
    s=[t["pos"] for t in s["tokens"]]
    pos=pos+(tuple(s))
    
pairs=()
for s in output["sentences"]:
    pairs=pairs+ (tuple(PhrasePairs(nltk.Tree.fromstring(s["parse"]))))

print("Parts of speech:")
print(pos)

print("Word pairs:")
print(pairs)
```

The parts of speech and pronouns are extracted as expected. And, another example:

```{python}
def PhrasePairs(tmp):
    tmp=tmp.productions()
    
    lst=[]
    for objects in tmp:
        objects=str(objects)
        objects=objects.split(" -> ")
        if('"' in str(objects)):
            continue
        lst.append(objects)
    
    pairs=[]
    for items in lst:
        parent=str(items[0])
        children=str(items[1]).split(" ")
        for child in children:
            hold="('"+parent +"', '" +child+"')"
            pairs.append(hold)
    return pairs
    
import pycorenlp as nlp
import nltk
nlp = nlp.StanfordCoreNLP('http://localhost:9000')

text="Nobody likes check chasers. The goal is to get the best players possible for as little money as possible. Bill Belichick doesn't have to settle for overpaying people though. Great players come to him for cheap, because they want to win."
output = nlp.annotate(text, properties={
        'annotators': 'tokenize,pos,parse',
        'outputFormat': 'json',
        'timeout': 10000
        })

tree=[s['parse'] for s in output['sentences']]
nltk.Tree.fromstring(tree[1]).pretty_print()

pos=()
for s in output["sentences"]:
    s=[t["pos"] for t in s["tokens"]]
    pos=pos+(tuple(s))
    
pairs=()
for s in output["sentences"]:
    pairs=pairs+ (tuple(PhrasePairs(nltk.Tree.fromstring(s["parse"]))))

print("Parts of speech:")
print(pos)

print("Word pairs:")
print(pairs)
```
Note that I only showed the parse tree from one of the four sentences, but the parts of speech and pairs are extracted for the whole comment correctly.

There is also another source of error--- mistakes made by the Stanford parser. But, since the Stanford parser is consistently used by the natural language processing community, we won't be too concerned with that for now.

##References

[1] Caliskan-Islam, Aylin, Richard Harang, Andrew Liu, Arvind Narayanan, Clare Voss, Fabian Yamaguchi, and Rachel Greenstadt. 2015. De-anonymizing programmers via code stylometry. In 24th USENIX Security Symposium (USENIX Security), Washington, DC.

[2] Narayanan, Arvind, Hristo Paskov, Neil Zhenqiang Gong, John Bethencourt, Emil Stefanov, Eui Chul Richard Shin, and Dawn Song. 2012. On the feasibility of internet-scale author identification. In Security and Privacy (SP), 2012 IEEE Symposium on, pp. 300-314.

[3] Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.